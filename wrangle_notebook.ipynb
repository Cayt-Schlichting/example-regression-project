{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26e4a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import wrangle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8791c5b",
   "metadata": {},
   "source": [
    "## Acquire Data\n",
    "\n",
    "Data acquired from 'zillow' database. Looking at the available rows and running a few basic queries in MySQL, the following decisions were made:\n",
    "- bathroomcnt and bedroomcnt will be used\n",
    "  - calculatedbathnbr and threequarterbathnbr had many nulls\n",
    "  - roomcnt was primarily zeros and its meaning is unclear.  this could be pursued in future analyses\n",
    "- taxamount won't be pulled in as it is calculated off of tax value, our target variable\n",
    "- regionidzip and fips will be used\n",
    "  - regionidneighborhood was half nulls and barely had more distinct entries than regionidzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write some SQL - expect 52441 rows\n",
    "sql = \"\"\"\n",
    "SELECT bedroomcnt as bed,\n",
    "    bathroomcnt as bath, \n",
    "    calculatedfinishedsquarefeet as sf, \n",
    "    taxvaluedollarcnt as value, \n",
    "    yearbuilt, \n",
    "    assessmentyear,\n",
    "    regionidzip as zipcode, \n",
    "    fips\n",
    "FROM properties_2017\n",
    "    JOIN propertylandusetype USING(propertylandusetypeid)\n",
    "    JOIN predictions_2017 USING(parcelid)\n",
    "WHERE propertylandusedesc = 'Single Family Residential' AND transactiondate LIKE '2017%%';\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e666e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_backup = pd.read_sql(sql,wrangle.get_db_url('zillow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c98d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape #rows match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4ac35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b44dd9",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "### Investigating Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c59b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's check out how the nulls overlap\n",
    "#Count how many values we have per row\n",
    "hist = df.count(axis=1,numeric_only=False)\n",
    "plt.hist(hist[hist<8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e098fca",
   "metadata": {},
   "source": [
    "Looking at the above, a decent number of these are only missing one or two pieces of data. However, since we are still only looking at ~140 total rows, I am going to go ahead and drop them all. Ideally, I'd check some of this with stakeholders. My primary concern is that we may be inadvertently trimming a particular geographic area (perhaps one with poor reporting on year built and zip code)\n",
    "\n",
    "**ACTION:** Drop all nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da653f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c374ed",
   "metadata": {},
   "source": [
    "### Look at the distributions of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc15427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See on logarythmic scale to better see outliers\n",
    "plt.figure(figsize=(10,20))\n",
    "ct=0\n",
    "for c in df.columns:\n",
    "    ct +=1\n",
    "    plt.subplot(8,2,ct)\n",
    "    plt.hist(df[c],bins=20)\n",
    "    plt.title(c+'_log')\n",
    "    plt.yscale('log')\n",
    "    ct +=1\n",
    "    plt.subplot(8,2,ct)\n",
    "    plt.hist(df[c],bins=20)\n",
    "    plt.title(c)\n",
    "    plt.ylim((0,100))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35179e2",
   "metadata": {},
   "source": [
    "Definitely some bed, bath, sf, value, assessment year and year outliers. I don't want to snap any values in. \n",
    "\n",
    "- Since Zillow estimates are not expected to be utilized by the top 1%, I want to cut some of the expensive and large houses. We'll cut the top .1% of sf homes off the dataset. NOTE: I recognized that sf is being used as a proxy for value, but it's one method of avoiding trimming by our target so that we can better specify how the model was trained in a useful manner\n",
    "- Since the data is skewed right, I want to trim less off the left. After considering a few different cutoffs, I found that california code restricts minimum dwelling size to 120 sq ft.\n",
    "  - \"Every dwelling unit shall have at least one room that shall have not less than 120 square feet (13.9 m2) of net floor area\"\n",
    "  \n",
    "**ACTION:** Drop rows with the top .1% of sf or an sf of less than 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce3714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop top 1% of sf\n",
    "df = df[df.sf<df.sf.quantile(.999)]\n",
    "\n",
    "#drop anything less than 120 sf\n",
    "df = df[df.sf>=120]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0167d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot again\n",
    "plt.figure(figsize=(10,15))\n",
    "ct=0\n",
    "for c in df.columns:\n",
    "    ct +=1\n",
    "    plt.subplot(8,2,ct)\n",
    "    plt.hist(df[c],bins=20)\n",
    "    plt.title(c+'_log')\n",
    "    plt.yscale('log')\n",
    "    ct +=1\n",
    "    plt.subplot(8,2,ct)\n",
    "    plt.hist(df[c],bins=20)\n",
    "    plt.title(c)\n",
    "    plt.ylim((0,50))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149fac2",
   "metadata": {},
   "source": [
    "Even after using sf trimming, we still have quite a few outliers in each category. Because of that, I'll do more trimming on the high side. Since Zillow's target customers aren't the super rich, I feel comfortable trimming 9+ bedrooms, 9+ bathrooms and 5+ million value. While we wanted to avoid any triming by value, it is import to get rid of these extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b74059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.bed > 8].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dd0b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.bath>8].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.value>5_000_000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19093fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see percent that fall in this category\n",
    "df[(df.value>5_000_000) | (df.bath >8) | (df.bed >8)].shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8be724",
   "metadata": {},
   "source": [
    "Even with the extra trimming, that only accounts for <1% of the data.\n",
    "\n",
    "**Action:** Drop all rows with 9+ beds, 9+ baths, or a value of 5+ million.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.value < 5_000_000) & (df.bath < 9) & (df.bed <9)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee43e7ea",
   "metadata": {},
   "source": [
    "Now let's look at assessment year & zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84464d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.assessmentyear.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba5b97",
   "metadata": {},
   "source": [
    "Since there was a transaction it looks to have a current assessment.\n",
    "\n",
    "**ACTION:** Drop the assessment year column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1442f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.zipcode.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9bc90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see if we don't have enough zipcode information for some\n",
    "df.zipcode.value_counts().hist(bins=100)\n",
    "plt.xlim(0,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf23bbc",
   "metadata": {},
   "source": [
    "There are a few zip codes with very few datapoints. \n",
    "\n",
    "**ACTION:** Drop all rows with a zip code that has less than 50 other properties in that zip code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_cnt = df.zipcode.value_counts()\n",
    "zip_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f38013",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_cnt[zip_cnt < 30].sum()/df_backup.shape[0] #1.3% of total dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b98ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "drp_zips = zip_cnt[zip_cnt < 30].index\n",
    "drp_zips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777534e",
   "metadata": {},
   "source": [
    "**WARNING:** I looked up some of these zip codes, and they do not correspond to the fips (counties) that are in this dataset.  Looking at the metadata, it states that these *should* be zip codes.  I will keep this info for now, as there is no reason to assume it is less or more valid than the fips.  It is possible it is another geographic identifier unique to zillow.\n",
    "\n",
    "Ideally, I would import this information into ArcGIS and plot the lat/lon of these properties and colorize them by the 'regionidzip'.  This would help to confirm they are geographic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b166ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows with those zip codes\n",
    "df = df[df.zipcode.isin(drp_zips)==False]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c74096",
   "metadata": {},
   "source": [
    "### Check dataypes are appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12509f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bed.value_counts() #can be integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20d5002",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.yearbuilt % 1).value_counts() # can be integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e6e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.sf %1).value_counts() # can be integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e39436",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.value %1).value_counts() # can be integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b7713",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.zipcode % 1 != 0] #good, all zips are whole numbers.  convert to string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5dd6bc",
   "metadata": {},
   "source": [
    "While sf and value can be floats, realistically they rarely are and the partial sf and dollars wouldn't matter.\n",
    "\n",
    "**ACTION:** Round down sf and value, then convert bed, yearbuilt, sf, value and zipcode to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdb6f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bed = df.bed.astype(int)\n",
    "df.yearbuilt = df.yearbuilt.astype(int)\n",
    "#astype automatically rounds floats\n",
    "df.sf = df.sf.astype(int)\n",
    "df.value = df.value.astype(int)\n",
    "#int first to get rid of the \".0\" then to string\n",
    "df.zipcode = df.zipcode.astype(int).astype(str)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d93a6",
   "metadata": {},
   "source": [
    "### Now create a new feature \n",
    "\n",
    "I want a new column that is squarefeet per # of bedrooms.\n",
    "\n",
    "Since the number of rooms is likely related to the squarefootage, I want to try and create a combined column.  Because bathrooms are generally much smaller than bedrooms, I don't want to include that in the numerator.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sf per bed column, consider 0 beds to have 1 bedroom (EX: studio)\n",
    "for i in df.index:\n",
    "    #for each row do math of sf/bed.  If bed is zero, use 1\n",
    "    df.loc[i,'sf_per_bed'] = df.loc[i,'sf']/ max(1,df.loc[i,'bed'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361474d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74272fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sf_per_bed.hist()\n",
    "plt.ylim(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ecee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.sf_per_bed < 200]\n",
    "#looks like we have an outlier - likely typo - index #48784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b366c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.sf_per_bed >3500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1b986b",
   "metadata": {},
   "source": [
    "**ACTION:** Drop the significant outliers in this column.  sf_per_bed < 100 and sf_per_bed >= 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b8542",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.sf_per_bed>99) & (df.sf_per_bed<3500)] #only gets rid of 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22085c2f",
   "metadata": {},
   "source": [
    "### Now encode the categorical variables, drop and reorder columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0bd7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to map then encode so that I have common sense \n",
    "# names for EDA and easy to read columns for the model\n",
    "\n",
    "#map to county names\n",
    "df['county'] = df.fips.map({6037: 'LosAngeles_CA',6059:'Orange_CA',6111:'Ventura_CA'})\n",
    "#encode into dummy df\n",
    "d_df = pd.get_dummies(df['county'],drop_first=True)\n",
    "#concat dummy df to the rest\n",
    "df = pd.concat([df,d_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cbd4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f5f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of original fips column \n",
    "df.drop(columns=['fips','assessmentyear'],inplace=True)\n",
    "#will also just not select assesment year in module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade08973",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reorder now, prior to encoding zipcode\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ac01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reorder columns with target and categorical in the front, encoded at the back\n",
    "df = df.reindex(columns=['value', 'zipcode', 'county', 'bed', 'bath', 'sf', 'sf_per_bed', 'yearbuilt', 'Orange_CA', 'Ventura_CA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026c258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49671288",
   "metadata": {},
   "source": [
    "##### zip code encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422f7304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In my function, I'll probably want to make including zip a parameter.\n",
    "#However, still want to encode for now\n",
    "#encode into dummy df\n",
    "dz_df = pd.get_dummies(df['zipcode'],drop_first=True)\n",
    "#concat dummy df to the rest\n",
    "df = pd.concat([df,dz_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01cfab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() #342 columns now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf206ec2",
   "metadata": {},
   "source": [
    "## Drop all into wrangle function in wrangle.py\n",
    "\n",
    "### Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f065c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = wrangle.getZillowData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e736cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 52441 entries, 0 to 52440\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   bed        52441 non-null  float64\n",
      " 1   bath       52441 non-null  float64\n",
      " 2   sf         52359 non-null  float64\n",
      " 3   value      52440 non-null  float64\n",
      " 4   yearbuilt  52325 non-null  float64\n",
      " 5   zipcode    52415 non-null  float64\n",
      " 6   fips       52441 non-null  float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 3.2 MB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29e29896",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, te, val = wrangle.prep_zillow(test_df,include_zip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe966077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 36448 entries, 17988 to 40159\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   value       36448 non-null  int64  \n",
      " 1   county      36448 non-null  object \n",
      " 2   bed         36448 non-null  int64  \n",
      " 3   bath        36448 non-null  float64\n",
      " 4   sf          36448 non-null  int64  \n",
      " 5   sf_per_bed  36448 non-null  float64\n",
      " 6   yearbuilt   36448 non-null  int64  \n",
      " 7   Orange_CA   36448 non-null  uint8  \n",
      " 8   Ventura_CA  36448 non-null  uint8  \n",
      "dtypes: float64(2), int64(4), object(1), uint8(2)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "tr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d442cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, te, val = wrangle.prep_zillow(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26e46190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['value', 'zipcode', 'county', 'bed', 'bath', 'sf', 'sf_per_bed',\n",
       "       'yearbuilt', 'Orange_CA', 'Ventura_CA', '95983', '95984'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.columns[0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08633b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 35984 entries, 33376 to 19456\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   value       35984 non-null  int64  \n",
      " 1   zipcode     35984 non-null  object \n",
      " 2   county      35984 non-null  object \n",
      " 3   bed         35984 non-null  int64  \n",
      " 4   bath        35984 non-null  float64\n",
      " 5   sf          35984 non-null  int64  \n",
      " 6   sf_per_bed  35984 non-null  float64\n",
      " 7   yearbuilt   35984 non-null  int64  \n",
      " 8   Orange_CA   35984 non-null  uint8  \n",
      " 9   Ventura_CA  35984 non-null  uint8  \n",
      " 10  95983       35984 non-null  uint8  \n",
      " 11  95984       35984 non-null  uint8  \n",
      "dtypes: float64(2), int64(4), object(2), uint8(4)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "tr[['value', 'zipcode', 'county', 'bed', 'bath', 'sf', 'sf_per_bed', 'yearbuilt',\n",
    "       'Orange_CA', 'Ventura_CA', '95983', '95984']].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e9cb2",
   "metadata": {},
   "source": [
    "### DATA PREP SUMMARY:\n",
    "- 2% of the data was dropped.  Rows were dropped if:\n",
    "  - There were any nulls\n",
    "  - It fell in the top 1% of square footage\n",
    "  - It had < 120 square feet \n",
    "  - There were less than 30 rows with the same zip code\n",
    "  - There were 9+ beds or 9+ baths\n",
    "  - The value was >= 5 million\n",
    "- Bed, yearbuilt, square footage and value were converted to integers\n",
    "- zipcode was converted to a string\n",
    "- fips was mapped to a readable column\n",
    "- zipcode and fips were encoded\n",
    "- Created a new column sf/bed\n",
    "  - bedrooms of 0 were treated as bed of 1\n",
    "  - major outliers were trimmed (only 3 rows)\n",
    "- columns were reorganized so that target and categorical were at the beginning, with encoded columns at the end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
